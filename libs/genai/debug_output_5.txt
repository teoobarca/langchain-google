/Users/hamper/Documents/Programming/nexana/langchain-google/.venv/lib/python3.14/site-packages/langsmith/schemas.py:22: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.
  from pydantic.v1 import (
============================= test session starts ==============================
platform darwin -- Python 3.14.0, pytest-9.0.1, pluggy-1.6.0
rootdir: /Users/hamper/Documents/Programming/nexana/langchain-google/libs/genai
configfile: pyproject.toml
plugins: anyio-4.11.0, langsmith-0.4.46
collected 3 items

tests/unit_tests/test_chat_models.py FFDEBUG: genai_types has Blob: True
DEBUG: genai_types has Content: True
DEBUG: part=media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=None inline_data=None text='I need to think...' thought=True thought_signature=b'test_sig_data' video_metadata=None, type=<class 'google.genai.types.Part'>
DEBUG: part=media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=None inline_data=None text='Final answer' thought=None thought_signature=b'test_sig_data' video_metadata=None, type=<class 'google.genai.types.Part'>
DEBUG: part=media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=None inline_data=None text='Follow up response' thought=None thought_signature=None video_metadata=None, type=<class 'google.genai.types.Part'>
F

=================================== FAILURES ===================================
________________ test_modalities_override_in_generation_config _________________

    def test_modalities_override_in_generation_config() -> None:
        """Test response modalities in invoke `generation_config` override model-defined."""
        from langchain_google_genai import Modality
    
        # Mock response with both image and text content
        mock_response = Mock()
        mock_response.candidates = [
            Candidate(
                content=Content(
                    parts=[
                        Part(
                            inline_data=genai_types.Blob(
                                mime_type="image/jpeg",
                                data=base64.b64decode(
                                    "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAABAAEDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAv/xAAUEAEAAAAAAAAAAAAAAAAAAAAA/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAX/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwCdABmX/9k="
                                ),
                            )
                        ),
                        Part(text="Meow! Here's a cat image for you."),
                    ]
                ),
                finish_reason=genai_types.FinishReason.STOP,
            )
        ]
        # Create proper usage metadata using dict approach
        # from google.ai.generativelanguage_v1beta.types import UsageMetadata
    
>       mock_response.usage_metadata = genai_types.GenerateContentResponseUsageMetadata.model_validate(
            {
                "prompt_token_count": 10,
                "response_token_count": 5,
                "total_token_count": 15,
            }
        )
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for GenerateContentResponseUsageMetadata
E       response_token_count
E         Extra inputs are not permitted [type=extra_forbidden, input_value=5, input_type=int]
E           For further information visit https://errors.pydantic.dev/2.12/v/extra_forbidden

tests/unit_tests/test_chat_models.py:2003: ValidationError
_______________ test_chat_google_genai_invoke_with_audio_mocked ________________

    def test_chat_google_genai_invoke_with_audio_mocked() -> None:
        """Test generating audio with mocked response and `content_blocks` translation."""
        mock_response = GenerateContentResponse.model_validate(
            {
                "candidates": [
                    {
                        # Empty content when audio is in additional_kwargs
                        "content": {"parts": []},
                        "finish_reason": "STOP",
                    }
                ],
                "usage_metadata": {
                    "prompt_token_count": 10,
                    "candidates_token_count": 5,
                    "total_token_count": 15,
                },
            }
        )
    
        wav_bytes = (  # (minimal WAV header)
            b"RIFF$\x00\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x01\x00"
            b"\x44\xac\x00\x00\x88X\x01\x00\x02\x00\x10\x00data\x00\x00\x00\x00"
        )
    
        llm = ChatGoogleGenerativeAI(
            model=MODEL_NAME,
            google_api_key=SecretStr(FAKE_API_KEY),
            response_modalities=[Modality.AUDIO],
        )
    
        with patch.object(llm.client.models, "generate_content", return_value=mock_response):
            with patch(
                "langchain_google_genai.chat_models._parse_response_candidate"
            ) as mock_parse:
                mock_parse.return_value = AIMessage(
                    content="",
                    additional_kwargs={"audio": wav_bytes},
                    usage_metadata={
                        "input_tokens": 10,
                        "output_tokens": 5,
                        "total_tokens": 15,
                    },
                    response_metadata={"model_provider": "google_genai"},
                )
    
>               result = llm.invoke(
                    "Please say The quick brown fox jumps over the lazy dog",
                )

tests/unit_tests/test_chat_models.py:2256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
langchain_google_genai/chat_models.py:1960: in invoke
    return super().invoke(input, config, stop=stop, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:398: in invoke
    self.generate_prompt(
../../.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1117: in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:927: in generate
    self._generate_with_cache(
../../.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1221: in _generate_with_cache
    result = self._generate(
langchain_google_genai/chat_models.py:2076: in _generate
    request = self._prepare_request(
langchain_google_genai/chat_models.py:2387: in _prepare_request
    config = self._prepare_params(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), response_modalities=[<Modality.AUDIO: 3>], client=<google.genai.client.Client object at 0x10af3b230>, default_metadata=(), model_kwargs={})
stop = None, generation_config = None, kwargs = {}, thinking_config = None
gen_config = {'candidate_count': 1, 'response_modalities': [<Modality.AUDIO: 3>], 'temperature': 0.7}
response_mime_type = None, response_schema = None, response_json_schema = None

    def _prepare_params(
        self,
        stop: list[str] | None,
        generation_config: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> genai_types.GenerateContentConfig:
        thinking_config = None
        if (
            self.thinking_budget is not None
            or self.include_thoughts is not None
            or self.thinking_level is not None
        ):
            thinking_config = genai_types.ThinkingConfig(
                thinking_budget=self.thinking_budget,
                include_thoughts=self.include_thoughts,
                thinking_level=self.thinking_level,
            )
    
        gen_config = {
            "candidate_count": self.n,
            "temperature": self.temperature,
            "stop_sequences": stop,
            "max_output_tokens": self.max_output_tokens,
            "top_k": self.top_k,
            "top_p": self.top_p,
            "response_modalities": self.response_modalities,
            "thinking_config": thinking_config,
        }
        # Filter None values
        gen_config = {k: v for k, v in gen_config.items() if v is not None}
    
        if generation_config:
            gen_config = {**gen_config, **generation_config}
    
        response_mime_type = kwargs.get("response_mime_type", self.response_mime_type)
        if response_mime_type is not None:
            gen_config["response_mime_type"] = response_mime_type
    
        response_schema = kwargs.get("response_schema", self.response_schema)
    
        # In case passed in as a direct kwarg
        response_json_schema = kwargs.get("response_json_schema")
    
        # Handle both response_schema and response_json_schema
        # (Regardless, we use `response_json_schema` in the request)
        schema_to_use = (
            response_json_schema
            if response_json_schema is not None
            else response_schema
        )
    
        if schema_to_use is not None:
            if response_mime_type != "application/json":
                param_name = (
                    "response_json_schema"
                    if response_json_schema is not None
                    else "response_schema"
                )
                error_message = (
                    f"'{param_name}' is only supported when "
                    f"response_mime_type is set to 'application/json'"
                )
                if response_mime_type == "text/x.enum":
                    error_message += (
                        ". Instead of 'text/x.enum', define enums using JSON schema."
                    )
                raise ValueError(error_message)
    
            gen_config["response_json_schema"] = schema_to_use
    
        media_resolution = kwargs.get("media_resolution", self.media_resolution)
        if media_resolution is not None:
            gen_config["media_resolution"] = media_resolution
    
>       return genai_types.GenerateContentConfig(**gen_config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       pydantic_core._pydantic_core.ValidationError: 1 validation error for GenerateContentConfig
E       response_modalities.0
E         Input should be a valid string [type=string_type, input_value=<Modality.AUDIO: 3>, input_type=GenerationConfig.Modality]
E           For further information visit https://errors.pydantic.dev/2.12/v/string_type

langchain_google_genai/chat_models.py:2059: ValidationError
_____________________ test_signature_round_trip_conversion _____________________

    def test_signature_round_trip_conversion() -> None:
        """Test complete round-trip signature handling in conversation context."""
    
        # Debug print
        print(f"DEBUG: genai_types has Blob: {'Blob' in dir(genai_types)}")
        print(f"DEBUG: genai_types has Content: {'Content' in dir(genai_types)}")
    
        # Create a mock response with thought signature
        binary_sig = b"test_sig_data"
        mock_response = GenerateContentResponse(
            candidates=[
                Candidate(
                    content=Content(
                        parts=[
                            Part(
                                text="I need to think...",
                                thought=True,
                                thought_signature=binary_sig,
                            ),
                            Part(text="Final answer", thought_signature=binary_sig),
                        ]
                    )
                )
            ]
        )
    
        with patch("langchain_google_genai.chat_models._chat_with_retry") as mock_chat:
            mock_chat.return_value = mock_response
    
            llm = ChatGoogleGenerativeAI(
                model=MODEL_NAME,
                google_api_key=SecretStr(FAKE_API_KEY),
                output_version="v1",
                include_thoughts=True,
            )
    
            # First call - get response with signatures
            result = llm.invoke("Test message")
    
            # Verify signatures were extracted
            assert isinstance(result.content, list)
    
            # Find blocks with signatures
            sig_blocks = []
            for block in result.content:
                if isinstance(block, dict):
                    if block.get("type") == "reasoning" and "signature" in block:
                        sig_blocks.append(block)
                    elif block.get("extras") and "signature" in block["extras"]:
                        sig_blocks.append(block)
    
            assert len(sig_blocks) >= 1, (
                f"Expected signature blocks, got content: {result.content}"
            )
    
            # Now simulate passing this result back in a conversation
            with patch(
                "langchain_google_genai.chat_models._convert_from_v1_to_generativelanguage_v1beta"
            ) as mock_convert:
                from langchain_google_genai._compat import (
                    _convert_from_v1_to_generativelanguage_v1beta as real_convert,
                )
    
                mock_convert.side_effect = real_convert
    
                # Create conversation with the signature-containing message
                conversation = [
                    HumanMessage(content="First message"),
                    result,  # This contains signatures
                    HumanMessage(content="Follow up"),
                ]
    
                # This should trigger signature conversion
                mock_chat.return_value = GenerateContentResponse(
                    candidates=[
                        Candidate(content=Content(parts=[Part(text="Follow up response")]))
                    ]
                )
    
                follow_up = llm.invoke(conversation)
    
                # Verify conversion was called
                assert mock_convert.call_count >= 1
    
                # Find calls with signatures
                calls_with_signatures = []
                for call in mock_convert.call_args_list:
                    content_blocks, model_provider = call[0]
                    if model_provider == "google_genai":
                        for block in content_blocks:
                            if isinstance(block, dict):
                                if block.get("type") == "reasoning" and block.get(
                                    "extras", {}
                                ).get("signature"):
                                    calls_with_signatures.append(call)
                                    break
                                if block.get("type") == "text" and block.get(
                                    "extras", {}
                                ).get("signature"):
                                    calls_with_signatures.append(call)
                                    break
    
>               assert len(calls_with_signatures) >= 1, (
                    "Expected at least one call to convert signatures"
                )
E               AssertionError: Expected at least one call to convert signatures
E               assert 0 >= 1
E                +  where 0 = len([])

tests/unit_tests/test_chat_models.py:2557: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  langchain_google_genai.chat_models:chat_models.py:497 Unrecognized message part format. Assuming it's a text part.
=============================== warnings summary ===============================
../../.venv/lib/python3.14/site-packages/_pytest/config/__init__.py:1397
  /Users/hamper/Documents/Programming/nexana/langchain-google/.venv/lib/python3.14/site-packages/_pytest/config/__init__.py:1397: PytestConfigWarning: Unknown config option: asyncio_mode
  
    self._warn_or_fail_if_strict(f"Unknown config option: {key}\n")

../../.venv/lib/python3.14/site-packages/google/genai/types.py:43
  /Users/hamper/Documents/Programming/nexana/langchain-google/.venv/lib/python3.14/site-packages/google/genai/types.py:43: DeprecationWarning: '_UnionGenericAlias' is deprecated and slated for removal in Python 3.17
    VersionedUnionType = Union[builtin_types.UnionType, _UnionGenericAlias]

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/unit_tests/test_chat_models.py::test_modalities_override_in_generation_config
FAILED tests/unit_tests/test_chat_models.py::test_chat_google_genai_invoke_with_audio_mocked
FAILED tests/unit_tests/test_chat_models.py::test_signature_round_trip_conversion
======================== 3 failed, 2 warnings in 0.70s =========================
