/Users/hamper/Documents/Programming/nexana/langchain-google/.venv/lib/python3.14/site-packages/langsmith/schemas.py:22: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.
  from pydantic.v1 import (
============================= test session starts ==============================
platform darwin -- Python 3.14.0, pytest-9.0.1, pluggy-1.6.0
rootdir: /Users/hamper/Documents/Programming/nexana/langchain-google/libs/genai
configfile: pyproject.toml
plugins: anyio-4.11.0, langsmith-0.4.46
collected 2 items

tests/unit_tests/test_chat_models.py F.

=================================== FAILURES ===================================
_ test_response_to_result_grounding_metadata[raw_response0-expected_grounding_metadata0] _

raw_response = {'candidates': [{'content': {'parts': [{'text': 'Test response'}]}, 'grounding_metadata': {'grounding_chunks': [{'web'... ['test query']}}], 'usage_metadata': {'candidates_token_count': 5, 'prompt_token_count': 10, 'total_token_count': 15}}
expected_grounding_metadata = {'grounding_chunks': [{'web': {'title': 'Example Site', 'uri': 'https://example.com'}}], 'grounding_supports': [{'conf... {'end_index': 13, 'part_index': 0, 'start_index': 0, 'text': 'Test response'}}], 'web_search_queries': ['test query']}

    @pytest.mark.parametrize(
        ("raw_response", "expected_grounding_metadata"),
        [
            (
                # Case 1: Response with grounding_metadata
                {
                    "candidates": [
                        {
                            "content": {"parts": [{"text": "Test response"}]},
                            "grounding_metadata": {
                                "grounding_chunks": [
                                    {
                                        "web": {
                                            "uri": "https://example.com",
                                            "title": "Example Site",
                                        }
                                    }
                                ],
                                "grounding_supports": [
                                    {
                                        "segment": {
                                            "start_index": 0,
                                            "end_index": 13,
                                            "text": "Test response",
                                            "part_index": 0,
                                        },
                                        "grounding_chunk_indices": [0],
                                        "confidence_scores": [0.95],
                                    }
                                ],
                                "web_search_queries": ["test query"],
                            },
                        }
                    ],
                    "usage_metadata": {
                        "prompt_token_count": 10,
                        "candidates_token_count": 5,
                        "total_token_count": 15,
                    },
                },
                {
                    "grounding_chunks": [
                        {"web": {"uri": "https://example.com", "title": "Example Site"}}
                    ],
                    "grounding_supports": [
                        {
                            "segment": {
                                "start_index": 0,
                                "end_index": 13,
                                "text": "Test response",
                                "part_index": 0,
                            },
                            "grounding_chunk_indices": [0],
                            "confidence_scores": [0.95],
                        }
                    ],
                    "web_search_queries": ["test query"],
                },
            ),
            (
                # Case 2: Response without grounding_metadata
                {
                    "candidates": [
                        {
                            "content": {"parts": [{"text": "Test response"}]},
                        }
                    ],
                    "usage_metadata": {
                        "prompt_token_count": 10,
                        "candidates_token_count": 5,
                        "total_token_count": 15,
                    },
                },
                {},
            ),
        ],
    )
    def test_response_to_result_grounding_metadata(
        raw_response: dict, expected_grounding_metadata: dict
    ) -> None:
        """Test that `_response_to_result` includes grounding_metadata in the response."""
        response = GenerateContentResponse.model_validate(raw_response)
        result = _response_to_result(response, stream=False)
    
        assert len(result.generations) == len(raw_response["candidates"])
        for generation in result.generations:
            grounding_metadata = (
                generation.generation_info.get("grounding_metadata", {})
                if generation.generation_info is not None
                else {}
            )
            assert grounding_metadata == expected_grounding_metadata
    
            # Check content format based on whether grounding metadata is present
            if expected_grounding_metadata:
                content_blocks = generation.message.content_blocks
                assert isinstance(content_blocks, list)
                assert len(content_blocks) == 1
                content_block = content_blocks[0]
                assert isinstance(content_block, dict)
                assert content_block["type"] == "text"
                assert content_block["text"] == "Test response"
                assert "annotations" in content_block
                assert len(content_block["annotations"]) == 1
                annotation = content_block["annotations"][0]
                assert annotation["type"] == "citation"
>               assert annotation["cited_text"] == "Test response"
                       ^^^^^^^^^^^^^^^^^^^^^^^^
E               KeyError: 'cited_text'

tests/unit_tests/test_chat_models.py:1442: KeyError
=============================== warnings summary ===============================
../../.venv/lib/python3.14/site-packages/_pytest/config/__init__.py:1397
  /Users/hamper/Documents/Programming/nexana/langchain-google/.venv/lib/python3.14/site-packages/_pytest/config/__init__.py:1397: PytestConfigWarning: Unknown config option: asyncio_mode
  
    self._warn_or_fail_if_strict(f"Unknown config option: {key}\n")

../../.venv/lib/python3.14/site-packages/google/genai/types.py:43
  /Users/hamper/Documents/Programming/nexana/langchain-google/.venv/lib/python3.14/site-packages/google/genai/types.py:43: DeprecationWarning: '_UnionGenericAlias' is deprecated and slated for removal in Python 3.17
    VersionedUnionType = Union[builtin_types.UnionType, _UnionGenericAlias]

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/unit_tests/test_chat_models.py::test_response_to_result_grounding_metadata[raw_response0-expected_grounding_metadata0]
=================== 1 failed, 1 passed, 2 warnings in 1.42s ====================
